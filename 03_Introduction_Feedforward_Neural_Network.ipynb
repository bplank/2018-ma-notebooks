{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After this lecture you should:\n",
    "* know what a neural network is\n",
    "* understand its basic building blocks\n",
    "* connect different views on neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A neural network is computational model that has slightly different meanings in different communitites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Cognitive science view**: a computational model of the brain consisting of artificial neural perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Machine Learning view**: \n",
    "   * **Linear algebra view**: a network of perceptron-like nodes, i.e., a set of matrix multiplication operations\n",
    "   * **Graph theory view**: a computational graph model (with automatic differentiation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As the name already suggests, a neural network is a network. It can be seen as a model that is build up from basic building blocks. Lets first look at one such building block, for instance, a single **perceptron**. \n",
    "\n",
    "<img src=\"pics/lego.jpg\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## From biological neurons to artificial neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To get started, I will first introduce a type of artificial neuron the **perceptron**. It was introduced with the well-known perceptron algorithm by Rosenblatt (1957), inspired by earlier work on McCulloch-Pitts to model neurons in the brain. In layman's terms, a neuron gets information through dendrites and if enough information is accumulated the neuron 'fires' and send information down the axon: \n",
    "\n",
    "<img src=\"pics/neuron.jpg\" width=\"350\" style=\"float: left\"><img src=\"pics/neuron-simple.png\" width=\"350\">\n",
    "\n",
    "Thus neural networks are biologically inspired. (But its overly simplistic..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How does the perceptron work?\n",
    "The basic perceptron gets **inputs** $x_1,..,x_n$ and produces an **output** $y$. It does so by **weighting** the inputs by $w_1,..,w_n$, sums up the weighted intputs and sends this weighted sum through an **activation function** $\\sigma$ to see if the neuron \"fires\". That is, if the weighted sum is above some **threshold** it will output 1, otherwise 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mathematically, the perceptron is formulated as: \n",
    "\n",
    "$y = \\sigma(\\sum_{j=1}^d w_{kj} x_j )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can visualize the perceptron as (for a given perceptron node $k$): <img src=\"pics/perceptron.png\" width=400> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is $\\sigma$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the perceptron $\\sigma$ is a **threshold** function. Intuitively, the perceptron only fires if the weighted sum is above some threshold. We can formulize this intuition as:\n",
    "\n",
    "\n",
    "$$\\begin{equation}\n",
    "    y=\n",
    "    \\begin{cases}\n",
    "      1 & \\text{if} (\\sum_j w_j x_j) > threshold\\\\\n",
    "      0 & \\text{otherwise}\\\\\n",
    "    \\end{cases}\n",
    "  \\end{equation}$$\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let us rewrite the equation of the perceptron. First, notice that $\\sum_{j=1} w_{j} x_j $ is the **dot product** of the weights and input, and can be written as: \n",
    "\n",
    "$$\\sum_{j=1} w_{j} x_j = \\vec{w} \\cdot \\vec{x}$$ where $\\vec{w}$ and $\\vec{x}$ are now vectors. If it is clear from context we avoid the explicit vector notation and simply write: $w \\cdot x$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Second, we will move the threshold inside the equation by introducing $b$ the bias term $b=-threshold$. Using these two changes, the equation rewrites as:\n",
    "\n",
    "$$\\begin{equation}\n",
    "    y=\n",
    "    \\begin{cases}\n",
    "      1 & \\text{if} (w \\cdot x + b) > 0)\\\\\n",
    "      0 & \\text{otherwise}\\\\\n",
    "    \\end{cases}\n",
    "  \\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Did you notice how close this is to a model we saw yesterday?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "Suppose we have a perceptron with two inputs, weights -2 and -2 and bias term 3. This is illustrated as: <img src=\"pics/nand-graph.png\">\n",
    "What function does this simple perceptron compute?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def compute(x1, x2):\n",
    "    a = x1*-2 + x2*-2 + 1 * 3\n",
    "    if a > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "x1=1\n",
    "x2=0\n",
    "print(compute(x1,x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vectorization\n",
    "Instead of this cumbersome notation, let us use **vectorization**. We represent our input instances as vectors, and the entire data as a matrix. Also the weights are now a vector. Why this change? Vectorization helps us to compute the dot product of many instances at once, use one big matrix multiplication, hence speeding up the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "inputs = np.array([[0,0],\n",
    "                   [0,1],\n",
    "                   [1,0],\n",
    "                   [1,1]])\n",
    "def compute(input_matrix):\n",
    "    W = [-2,-2]\n",
    "    b = 3\n",
    "    a = np.dot(input_matrix,W) + b\n",
    "    return [1 if elem > 0 else 0 for elem in a]\n",
    "labels=compute(inputs)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Beautiful! Now we have a perceptron that models the NAND logical function. That is, it return 1 only if both inputs are active (not AND)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualization\n",
    "\n",
    "We can visualize the example by looking at where the input vectors are in the space and which label they get. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"pics/nand-plot.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Exercise (try at home):** try to manually modify the parameters (the weights and bias) of the little perceptron network. Can you get another logical function? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105f67d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "colors=['r','b']\n",
    "types=['x','o']\n",
    "for i, lab in enumerate(labels):\n",
    "    plt.plot(inputs[i,0], inputs[i,1], types[lab], color=colors[lab])\n",
    "plt.axis([-0.5, 1.5, -0.5, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall the perceptron formula, what does it resemble?\n",
    "\n",
    "$$\\begin{equation}\n",
    "    y=\n",
    "    \\begin{cases}\n",
    "      1 & \\text{if} (w \\cdot x + b) > 0)\\\\\n",
    "      0 & \\text{otherwise}\\\\\n",
    "    \\end{cases}\n",
    "  \\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Right, it's the equation of a line! To be precise, since the inputs have usually more than 2 dimensions it is actually a **hyperplane**. Dry to imagine the line in our NAND example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear separability\n",
    "\n",
    "The perceptron is a **linear** classifier. (Remember again the formula!). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Linearly separable?\n",
    "Now have a look at the following two examples. Are they linearly separable? (hint: Which logical functions do they represent?) \n",
    "<img src=\"pics/linearq.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "left: OR, right: XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Right. And this is a limitation of the perceptron. If the data is not linearly separable, the perceptron has a hard time. So what can we do about it? There are tricks to make the perceptron work in such cases, but usually you will move to a model with higher **representational capacity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/separability.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why Neural Networks\n",
    "\n",
    "* **non-linearity**\n",
    "* **representational power** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A neural network is a **network** of nodes. It has **input** nodes, **output node(s)** and usually **hidden nodes**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Below is a visualization of a neural network. The green nodes are the inputs, the blue nodes are **hidden nodes** and the last *layer* is the **output** layer. How many input, hidden and output nodes does this network have?\n",
    "\n",
    "A feedforward neural network:\n",
    "<img src=\"pics/nn.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Or another visualization: <img src=\"http://neuralnetworksanddeeplearning.com/images/tikz11.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Such a basic neural network is also called:\n",
    "* **feedforward neural network**\n",
    "* **multi-layer perceptron** (MLP) (for some odd historical reasons)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lets look at a more detailed example. The network  can be formulized as:\n",
    "<img src=\"pics/nn.png\" width=300> \n",
    "\n",
    "$$NN_{MLP1}(\\mathbf{x})=g(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2}$$\n",
    "\n",
    "there $g$ is a non-linearity/activation function. We will come back to this later. For now, discuss with your neighbor: what are all the terms in the formula above, and how can you connect them to the picture above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A feedforward neural network with 2 hidden layers:\n",
    "$$NN_{MLP2}(\\mathbf{x})=g^2(g^1(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2})\\mathbf{W^3}+\\mathbf{b^3}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The MLP2 is illustrated here (vertically). \n",
    "<img src=\"pics/nn_vertical.png\" width=300>\n",
    "It is a bit cumbersome to see, so lets break the formula \n",
    "\n",
    "$$NN_{MLP2}(\\mathbf{x})=g^2(g^1(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2})\\mathbf{W^3}+\\mathbf{b^3}$$\n",
    "\n",
    "down into parts:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\mathbf{h^1}=g^1(\\mathbf{xW^1+b^1})$$\n",
    "$$\\mathbf{h^2}=g^2(\\mathbf{h_1W^2+b^2})$$\n",
    "$$NN_{MLP2}(\\mathbf{x})= \\mathbf{h^2}\\mathbf{W^3}+\\mathbf{b^3}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "## Notes on vector/matrix sizes:\n",
    "## x: 1x4 vector\n",
    "## W1: 4x6 matrix\n",
    "## b1: 1x6 vector \n",
    "## -> h1: 1x6   # first layer transforms 4 dimensional input into 6 dimensional vector, etc\n",
    "## W2: 6x5 matrix\n",
    "## b2: 1x5 vector \n",
    "## -> h2: 1x5\n",
    "## W3: 5x3 matrix\n",
    "## b3: 1x3 vector\n",
    "## -> output: 1x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Voila! Now we have a wonderful description of a neural network, both graphically and algebraically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### References\n",
    "* More details in [Michael Nielsen's book chapter 1](http://neuralnetworksanddeeplearning.com/chap1.html)\n",
    "* Yoav Goldberg's tutorial: [A Primer on Neural Network Models for Natural Language Processing](http://arxiv.org/abs/1510.00726)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
