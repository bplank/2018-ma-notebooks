{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks in `DyNet`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After this lecture you will:\n",
    "    \n",
    "* know how to implement a neural network in DyNet\n",
    "* understand pros/cons of static vs dynamic neural network libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Language is discrete and structured. Structure is ubiquitous in Natural Language Processing.\n",
    "\n",
    "- Sequences, trees, graphs\n",
    "\n",
    "<img src=\"pics/motivation.png\">\n",
    "\n",
    "Neural nets represent things with continuous vectors (we will see this in the next notebook)\n",
    "\n",
    "- Poor “native support” for structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## Two software modes\n",
    "\n",
    "### Static declaration\n",
    "\n",
    "1. Define an architecture\n",
    "\n",
    "2. Run a bunch of data through it to train the\n",
    "model and/or make predictions\n",
    "\n",
    "### Dynamic declaration\n",
    "\n",
    "Graph is defined implicitly as the forward computation is executed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Static declaration\n",
    "\n",
    "#### Pros\n",
    "\n",
    "- Offline optimization is powerful \n",
    "- Limits on operations mean better hardware support\n",
    "\n",
    "#### Cons\n",
    "\n",
    "- Structured data (even simple stuff like sequences), even variable-sized\n",
    "data, is ugly\n",
    "\n",
    "Examples: Tensorflow, Theano, Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dynamic declaration\n",
    "\n",
    "#### Pros\n",
    "\n",
    "- easier, flexibler modeling of structured problems\n",
    "- easier to work with variable-sized data (less ugly)\n",
    "\n",
    "#### Cons\n",
    "\n",
    "- if graph is static, dynamic effort is wasted\n",
    "- less space for optimization\n",
    "\n",
    "Examples: DyNet, PyTorch, Chainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`DyNet` offers flexibility in modeling your problem with a dynamic neural network library.\n",
    "\n",
    "As Chris, Yoav and Graham put it: \"*Things that are cumbersome / hard / ugly in other\n",
    "frameworks*\" [and that's where DyNet shines...] ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will use the `python` wrapper of `DyNet`. Originally it is implemented as C++ library based on eigen (like Tensorflow).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks in `DyNet`\n",
    "\n",
    "### The major ingredients that we need:\n",
    "\n",
    "- the computational graph abstraction\n",
    "\n",
    "- expressions\n",
    "\n",
    "- parameters\n",
    "\n",
    "- a model (i.e. collection of parameters )\n",
    "\n",
    "- a trainer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expression 5/1\n",
      "[  1.   2.   3.   4.   5.   6.   7.   8.   6.   8.  10.  12.   2.   3.   4.\n",
      "   5.]\n"
     ]
    }
   ],
   "source": [
    "import dynet as dy\n",
    "dy.renew_cg() # create a new computation graph\n",
    "v1 = dy.inputVector([1,2,3,4])\n",
    "v2 = dy.inputVector([5,6,7,8])\n",
    "# v1 and v2 are expressions\n",
    "v3 = v1 + v2\n",
    "v4 = v3 * 2\n",
    "v5 = v1 + 1\n",
    "v6 = dy.concatenate([v1,v2,v3,v5])\n",
    "print(v6) # is a DyNet expression\n",
    "print(v6.npvalue()) # access its value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 6.0, 8.0, 10.0, 12.0, 2.0, 3.0, 4.0, 5.0]\n"
     ]
    }
   ],
   "source": [
    "print(v6.value()) #  alternative way to access its value (simple list, not numpy array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You create basic expressions, and combine them using *operations*. \n",
    "\n",
    "Expressions represent **symbolic computations**.\n",
    "\n",
    "To perform actual computations you use:\n",
    "\n",
    "```\n",
    ".value()\n",
    ".npvalue()\n",
    ".forward()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Models and Parameters\n",
    "\n",
    "- **Parameters** are the things that we optimize over\n",
    "(vectors, matrices).\n",
    "- **Model** is a collection of parameters.\n",
    "- Parameters **out-live** the computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = dy.Model()\n",
    "pW = model.add_parameters((20,4))\n",
    "pb = model.add_parameters(20)\n",
    "dy.renew_cg()\n",
    "x = dy.inputVector([1,2,3,4])\n",
    "W = dy.parameter(pW) # convert params to expression\n",
    "b = dy.parameter(pb) # and add to the graph\n",
    "y = W * x + b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## different initializations\n",
    "\n",
    "model = dy.Model()\n",
    "pW = model.add_parameters((4,4))\n",
    "pW2 = model.add_parameters((4,4), init=dy.GlorotInitializer())\n",
    "pW3 = model.add_parameters((4,4), init=dy.NormalInitializer(0,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Trainers and Backprop\n",
    "\n",
    "- Initialize a Trainer with a given model.\n",
    "- Compute gradients by calling `expr.backward()`\n",
    "from a scalar node.\n",
    "- Call `trainer.update()` to update the model\n",
    "parameters using the gradients.\n",
    "- There are many different training algorithms available (check the docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = dy.Model()\n",
    "trainer = dy.SimpleSGDTrainer(model)\n",
    "p_v = model.add_parameters(10)\n",
    "for i in range(10):\n",
    "     dy.renew_cg()\n",
    "     v = dy.parameter(p_v)\n",
    "     v2 = dy.dot_product(v,v)\n",
    "     v2.forward()\n",
    "     v2.backward() # compute gradients\n",
    "     trainer.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Training with `DyNet`\n",
    "\n",
    "- Create model, add parameters, create trainer.\n",
    "- For each training example:\n",
    "    - create computation graph for the loss\n",
    "    - run forward (compute the loss)\n",
    "    - run backward (compute the gradients)\n",
    "    - update parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: MLP for NAND\n",
    "\n",
    "Model: $$\\hat{y} = \\sigma( W x + b )$$ \n",
    "\n",
    "Data:\n",
    "\n",
    "```\n",
    "x1 x2  y\n",
    "0 0  1\n",
    "0 1  1\n",
    "1 0  1\n",
    "1 1  0\n",
    "```\n",
    "\n",
    "Loss (binary log loss): \n",
    "\n",
    "\n",
    "if $y=1$ then loss = $-\\log \\hat{y}$\n",
    "\n",
    "if $y=0$ then loss = $-\\log (1-\\hat{y})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 100 loss: 0.03673703083768487\n",
      "Iter: 200 loss: 0.012332652062468696\n",
      "Iter: 300 loss: 0.0069259492611308815\n",
      "Iter: 400 loss: 0.004703131926362403\n",
      "Iter: 500 loss: 0.003518811388858012\n",
      "Iter: 600 loss: 0.0027918846099055372\n",
      "Iter: 700 loss: 0.002303714075424068\n",
      "Iter: 800 loss: 0.0019548175596355577\n",
      "Iter: 900 loss: 0.001693963927209552\n"
     ]
    }
   ],
   "source": [
    "import dynet as dy\n",
    "import random\n",
    "data =[ ([0,1],1),\n",
    "        ([1,0],1),\n",
    "        ([0,0],1),\n",
    "        ([1,1],0) ]\n",
    "model = dy.Model()\n",
    "\n",
    "HIDDEN_SIZE = 8\n",
    "INPUT_SIZE = 2\n",
    "pU = model.add_parameters((HIDDEN_SIZE,INPUT_SIZE))\n",
    "pb = model.add_parameters(HIDDEN_SIZE)\n",
    "pv = model.add_parameters((1, HIDDEN_SIZE))\n",
    "\n",
    "trainer = dy.SimpleSGDTrainer(model)\n",
    "\n",
    "for ITER in range(1000):\n",
    "    closs = 0.0\n",
    "    random.shuffle(data)\n",
    "    for x,y in data:\n",
    "        dy.renew_cg()\n",
    "        W = dy.parameter(pU)\n",
    "        b = dy.parameter(pb)\n",
    "        V = dy.parameter(pv)\n",
    "        x = dy.inputVector(x)\n",
    "        # predict\n",
    "        h = dy.tanh((W*x) + b)\n",
    "        yhat = dy.logistic((V*h))\n",
    "        # loss\n",
    "        if y == 0:\n",
    "            loss = -dy.log(1 - yhat)\n",
    "        elif y == 1:\n",
    "            loss = -dy.log(yhat)\n",
    "\n",
    "        closs += loss.scalar_value() # forward\n",
    "        loss.backward()\n",
    "        trainer.update()\n",
    "        \n",
    "    if ITER > 0 and ITER % 100 == 0:\n",
    "        print(\"Iter:\",ITER,\"loss:\", closs/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us organize the code a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 100 loss: 0.04411190759856254\n",
      "Iter: 200 loss: 0.01389462522638496\n",
      "Iter: 300 loss: 0.0076934145981795155\n",
      "Iter: 400 loss: 0.005188286460906966\n",
      "Iter: 500 loss: 0.0038655066655337578\n",
      "Iter: 600 loss: 0.0030569497484975727\n",
      "Iter: 700 loss: 0.0025156566480291076\n",
      "Iter: 800 loss: 0.0021298859664966585\n",
      "Iter: 900 loss: 0.0018419274238112848\n"
     ]
    }
   ],
   "source": [
    "import dynet as dy\n",
    "import random\n",
    "data =[ ([0,1],1),\n",
    "        ([1,0],1),\n",
    "        ([0,0],1),\n",
    "        ([1,1],0) ]\n",
    "model = dy.Model()\n",
    "\n",
    "HIDDEN_SIZE = 8\n",
    "INPUT_SIZE = 2\n",
    "pU = model.add_parameters((HIDDEN_SIZE,INPUT_SIZE))\n",
    "pb = model.add_parameters(HIDDEN_SIZE)\n",
    "pv = model.add_parameters((1, HIDDEN_SIZE))\n",
    "\n",
    "trainer = dy.SimpleSGDTrainer(model)\n",
    "\n",
    "def calc_score(x):\n",
    "    W = dy.parameter(pU)\n",
    "    b = dy.parameter(pb)\n",
    "    V = dy.parameter(pv)\n",
    "    x = dy.inputVector(x)\n",
    "    # predict\n",
    "    h = dy.tanh((W*x) + b)\n",
    "    return dy.logistic((V*h))\n",
    "\n",
    "def my_loss_function(yhat, y):\n",
    "    if y == 0:\n",
    "        loss = -dy.log(1 - yhat)\n",
    "    elif y == 1:\n",
    "        loss = -dy.log(yhat)\n",
    "    return loss\n",
    "\n",
    "for ITER in range(1000):\n",
    "    closs = 0.0\n",
    "    random.shuffle(data)\n",
    "    for x,y in data:\n",
    "        dy.renew_cg()\n",
    "        # predict\n",
    "        yhat = calc_score(x)\n",
    "        # loss\n",
    "        loss = my_loss_function(yhat, y)\n",
    "\n",
    "        closs += loss.scalar_value() # record loss\n",
    "        loss.backward() \n",
    "        trainer.update()\n",
    "        \n",
    "    if ITER > 0 and ITER % 100 == 0:\n",
    "        print(\"Iter:\",ITER,\"loss:\", closs/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using a built-in loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 100 loss: 0.035967539937701076\n",
      "Iter: 200 loss: 0.012440204518497922\n",
      "Iter: 300 loss: 0.0070557887866016245\n",
      "Iter: 400 loss: 0.004808427196621778\n",
      "Iter: 500 loss: 0.0036034677605130128\n",
      "Iter: 600 loss: 0.00286049308397196\n",
      "Iter: 700 loss: 0.002360385213250993\n",
      "Iter: 800 loss: 0.002002454174544255\n",
      "Iter: 900 loss: 0.0017345331739306857\n"
     ]
    }
   ],
   "source": [
    "import dynet as dy\n",
    "import random\n",
    "data =[ ([0,1],1),\n",
    "        ([1,0],1),\n",
    "        ([0,0],1),\n",
    "        ([1,1],0) ]\n",
    "model = dy.Model()\n",
    "HIDDEN_SIZE = 8\n",
    "INPUT_SIZE = 2\n",
    "pU = model.add_parameters((HIDDEN_SIZE,INPUT_SIZE))\n",
    "pb = model.add_parameters(HIDDEN_SIZE)\n",
    "pv = model.add_parameters((1, HIDDEN_SIZE))\n",
    "trainer = dy.SimpleSGDTrainer(model)\n",
    "\n",
    "def calc_score(x):\n",
    "    W = dy.parameter(pU)\n",
    "    b = dy.parameter(pb)\n",
    "    V = dy.parameter(pv)\n",
    "    x = dy.inputVector(x)\n",
    "    # predict\n",
    "    h = dy.tanh((W*x) + b)\n",
    "    return dy.logistic((V*h))\n",
    "\n",
    "for ITER in range(1000):\n",
    "    closs = 0.0\n",
    "    random.shuffle(data)\n",
    "    for x,y in data:\n",
    "        dy.renew_cg()\n",
    "        # predict\n",
    "        yhat = calc_score(x)\n",
    "        # loss\n",
    "        loss = dy.binary_log_loss(yhat, dy.scalarInput(y))\n",
    "\n",
    "        closs += loss.scalar_value() # record loss\n",
    "        loss.backward() \n",
    "        trainer.update()\n",
    "        \n",
    "    if ITER > 0 and ITER % 100 == 0:\n",
    "        print(\"Iter:\",ITER,\"loss:\", closs/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "This notebook is heavily based on:\n",
    "- Chris Dyer, Yoav Goldberg, Graham Neubig: [DyNet tutorial - part 1](http://demo.clab.cs.cmu.edu/cdyer/emnlp2016-dynet-tutorial-part1.pdf) and [part 2](http://demo.clab.cs.cmu.edu/cdyer/emnlp2016-dynet-tutorial-part2.pdf) [awesome resource!]"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
